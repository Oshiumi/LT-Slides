### ニューラルネットワーク超入門 理論編

Monolith 鴛海太一

---

### ニューラルネットワークって？

動物が持つ脳神経系をまねた数理モデル
つまり、
**人間の脳が凄いのでまねすれば凄いんじゃね？**
ってこと

---

### 脳の構造

脳は **ニューロン** と呼ばれる神経細胞が **シナプス** と呼ばれる結合部位を介して結合している

---

### パーセプトロン

---?image=images/perceptron.png&size=auto 90%

Note:
a(x)は負の値を0，それ以外を1とするしきい関数．
(この関数a()のことを活性化関数または出力関数という)

---

### 階層型ニューラルネットワーク

---?image=images/hierarchical.png&size=auto 90%

Note:
入力層，中間層（隠れ層），出力層

---

### 学習

- そもそも学習とは                          |
 - さっきの重みをちょうどいい感じに調整すること |

- 教師あり学習                              |
 - 入力と出力のペアを渡して学習                 |
- 教師なし学習                                  |
 - 入力データのみで学習                            |

---

### 教師あり学習

- 誤り訂正学習 |
- 誤差逆伝播法 |

---

### 誤り訂正学習

- 入力x,　教師データt, 現在の重みwを使って出力yを計算
- 教師データと出力yが同じでない場合以下の式で重みwを更新
 $$w \gets w + \eta(t_j-y)x_j$$

Note:
つまり、間違っちゃったら少しずつ直して行こうねという優しい学習
こんな感じで少しずつ学習するのをオンライン学習
全ての学習データを通して重みの修正量を計算し、その総和をとってまとめて更新するのをバッチ処理という
全てではなくいくつかに分割して、その単位でまとめて更新するミニバッチもある

---

### 誤り訂正学習の問題点

- 教師データと出力を使うので出力層のノードへの結合の重みしか修正できない
- 出力層の前に連合層と呼ばれる重みが固定された層を作ったとしても、情報処理能力に限界がある

Note:
ここまで1960年の話

---

### 誤差逆伝播法

- Backpropagation method
- 活性化関数を微分可能なものに変更
 - これによって出力層での誤差評価から偏微分することによってかく重みの寄与を計算することが可能に
 - よく使われるのはシグモイド関数
 $$sig(x)=\frac {1} {1+e^{-x}}$$

Note:
誤差評価には二乗誤差などが用いられる(この関数を損失関数などという)
誤差逆伝播法自体は重みを微分して計算する方法というだけ
実際の重みの更新は次

---

### 誤差逆伝播法 重みの更新

- 誤差逆伝播法自体は微分可能な活性化関数を使うというだけ
- 重みの修正は勾配降下法などの最適化手法によって行う

---

### 確率的勾配降下法

$$w\_{k,j}^{n+1} \gets w\_{k,j}^{n+1} - \eta \frac {\partial R(W)} {\partial w\_{k,j}^{(n+1)}}$$
損失関数を
$$R(x)= \frac {1}{2} \sum\_j (t\_j - y\_j^{(N)})^2$$
とすると
$$w\_{k,j}^{(n+1)} \gets w\_{k,j}^{(n+1)} - \eta \delta\_j^{(n+1)}y\_k^{(n)}$$
ここで $y\_k^{(n)}$ は第n層の第kノードの出力


---

### 確率的勾配降下法

ここで
$$\delta\_j^{(N)}=-(t\_j - y\_j^{(N)})y\_j^{(N)} (a-y\_j^{(N)})$$
$$\delta\_j^{(N)}=\{\sum\_{k=1}^{K\_{n+1}}\delta\_j^{(n+1)}w\_{k,j}^{(n+1)}\}y\_j^{(n)}(1-y\_j^{(n)})$$
$K\_{n+1}$は第n+1層のノードの数
